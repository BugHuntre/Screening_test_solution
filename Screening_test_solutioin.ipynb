{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c7af47e-fb94-4393-9ec4-20c961667fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed Configuration:\n",
      "Target: petal_width\n",
      "Prediction Type: regression\n",
      "Numerical Features: ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
      "Categorical Features: ['species']\n",
      "Imputation Strategies: {'sepal_length': 'mean', 'sepal_width': 'mean', 'petal_length': 'mean', 'petal_width': 'mean'}\n",
      "Feature Reduction: {'feature_reduction_method': 'Tree-based', 'num_of_features_to_keep': '4', 'num_of_trees': '5', 'depth_of_trees': '6'}\n",
      "Algorithms: {'RandomForestClassifier': {'model_name': 'Random Forest Classifier', 'is_selected': False, 'min_trees': 10, 'max_trees': 30, 'feature_sampling_statergy': 'Default', 'min_depth': 20, 'max_depth': 30, 'min_samples_per_leaf_min_value': 5, 'min_samples_per_leaf_max_value': 50, 'parallelism': 0}, 'RandomForestRegressor': {'model_name': 'Random Forest Regressor', 'is_selected': True, 'min_trees': 10, 'max_trees': 20, 'feature_sampling_statergy': 'Default', 'min_depth': 20, 'max_depth': 25, 'min_samples_per_leaf_min_value': 5, 'min_samples_per_leaf_max_value': 10, 'parallelism': 0}, 'GBTClassifier': {'model_name': 'Gradient Boosted Trees', 'is_selected': False, 'num_of_BoostingStages': [67, 89], 'feature_sampling_statergy': 'Fixed number', 'learningRate': [], 'use_deviance': True, 'use_exponential': False, 'fixed_number': 22, 'min_subsample': 1, 'max_subsample': 2, 'min_stepsize': 0.1, 'max_stepsize': 0.5, 'min_iter': 20, 'max_iter': 40, 'min_depth': 5, 'max_depth': 7}, 'GBTRegressor': {'model_name': 'Gradient Boosted Trees', 'is_selected': False, 'num_of_BoostingStages': [67, 89], 'feature_sampling_statergy': 'Fixed number', 'use_deviance': True, 'use_exponential': False, 'fixed_number': 22, 'min_subsample': 1, 'max_subsample': 2, 'min_stepsize': 0.1, 'max_stepsize': 0.5, 'min_iter': 20, 'max_iter': 40, 'min_depth': 5, 'max_depth': 7}, 'LinearRegression': {'model_name': 'LinearRegression', 'is_selected': False, 'parallelism': 2, 'min_iter': 30, 'max_iter': 50, 'min_regparam': 0.5, 'max_regparam': 0.8, 'min_elasticnet': 0.5, 'max_elasticnet': 0.8}, 'LogisticRegression': {'model_name': 'LogisticRegression', 'is_selected': False, 'parallelism': 2, 'min_iter': 30, 'max_iter': 50, 'min_regparam': 0.5, 'max_regparam': 0.8, 'min_elasticnet': 0.5, 'max_elasticnet': 0.8}, 'RidgeRegression': {'model_name': 'RidgeRegression', 'is_selected': False, 'regularization_term': 'Specify values to test', 'min_iter': 30, 'max_iter': 50, 'min_regparam': 0.5, 'max_regparam': 0.8}, 'LassoRegression': {'model_name': 'Lasso Regression', 'is_selected': False, 'regularization_term': 'Specify values to test', 'min_iter': 30, 'max_iter': 50, 'min_regparam': 0.5, 'max_regparam': 0.8}, 'ElasticNetRegression': {'model_name': 'Lasso Regression', 'is_selected': False, 'regularization_term': 'Specify values to test', 'min_iter': 30, 'max_iter': 50, 'min_regparam': 0.5, 'max_regparam': 0.8, 'min_elasticnet': 0.5, 'max_elasticnet': 0.8}, 'xg_boost': {'model_name': 'XG Boost', 'is_selected': False, 'use_gradient_boosted_tree': True, 'dart': True, 'tree_method': '', 'random_state': 0, 'max_num_of_trees': 0, 'early_stopping': True, 'early_stopping_rounds': 2, 'max_depth_of_tree': [56, 89], 'learningRate': [89, 76], 'l1_regularization': [77], 'l2_regularization': [78], 'gamma': [68], 'min_child_weight': [67], 'sub_sample': [67], 'col_sample_by_tree': [67], 'replace_missing_values': False, 'parallelism': 0}, 'DecisionTreeRegressor': {'model_name': 'Decision Tree', 'is_selected': False, 'min_depth': 4, 'max_depth': 7, 'use_gini': False, 'use_entropy': True, 'min_samples_per_leaf': [12, 6], 'use_best': True, 'use_random': True}, 'DecisionTreeClassifier': {'model_name': 'Decision Tree', 'is_selected': False, 'min_depth': 4, 'max_depth': 7, 'use_gini': False, 'use_entropy': True, 'min_samples_per_leaf': [12, 6], 'use_best': True, 'use_random': True}, 'SVM': {'model_name': 'Support Vector Machine', 'is_selected': False, 'linear_kernel': True, 'rep_kernel': True, 'polynomial_kernel': True, 'sigmoid_kernel': True, 'c_value': [566, 79], 'auto': True, 'scale': True, 'custom_gamma_values': True, 'tolerance': 7, 'max_iterations': 7}, 'SGD': {'model_name': 'Stochastic Gradient Descent', 'is_selected': False, 'use_logistics': True, 'use_modified_hubber_loss': False, 'max_iterations': False, 'tolerance': 56, 'use_l1_regularization': 'on', 'use_l2_regularization': 'on', 'use_elastic_net_regularization': True, 'alpha_value': [79, 56], 'parallelism': 1}, 'KNN': {'model_name': 'KNN', 'is_selected': False, 'k_value': [78], 'distance_weighting': True, 'neighbour_finding_algorithm': 'Automatic', 'random_state': 0, 'p_value': 0}, 'extra_random_trees': {'model_name': 'Extra Random Trees', 'is_selected': False, 'num_of_trees': [45, 489], 'feature_sampling_statergy': 'Square root and Logarithm', 'max_depth': [12, 45], 'min_samples_per_leaf': [78, 56], 'parallelism': 3}, 'neural_network': {'model_name': 'Neural Network', 'is_selected': False, 'hidden_layer_sizes': [67, 89], 'activation': '', 'alpha_value': 0, 'max_iterations': 0, 'convergence_tolerance': 0, 'early_stopping': True, 'solver': 'ADAM', 'shuffle_data': True, 'initial_learning_rate': 0, 'automatic_batching': True, 'beta_1': 0, 'beta_2': 0, 'epsilon': 0, 'power_t': 0, 'momentum': 0, 'use_nesterov_momentum': False}}\n",
      "==================================================\n",
      "Processing Algorithm: RandomForestRegressor\n",
      "Starting Grid Search for RandomForestRegressor...\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "Best Parameters for RandomForestRegressor: {'model__max_depth': 20, 'model__min_samples_leaf': 5, 'model__n_estimators': 20}\n",
      "Model Evaluation Metrics:\n",
      "Mean Absolute Error (MAE): 0.030327390868934937\n",
      "Mean Squared Error (MSE): 0.0016741983393664245\n",
      "R-squared (R2): 0.9973661862549078\n",
      "------------------------------\n",
      "Saved best model to RandomForestRegressor_model.pkl\n",
      "==================================================\n",
      "All selected models have been processed and evaluated.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import joblib\n",
    "\n",
    "# Custom Transformer for Feature Generation (Dummy Example)\n",
    "class FeatureGenerator(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Implement feature generation based on config\n",
    "        # This is a placeholder. Implement actual feature generation logic as needed.\n",
    "        return X\n",
    "\n",
    "# Function to load JSON configuration from a file\n",
    "def load_config_from_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "# Corrected parse_config function\n",
    "def parse_config(config):\n",
    "    try:\n",
    "        design_data = config['design_state_data']\n",
    "        \n",
    "        # Extract target information\n",
    "        target_info = design_data['target']\n",
    "        target = target_info['target']\n",
    "        prediction_type = target_info['prediction_type'].lower()  # e.g., 'regression'\n",
    "        \n",
    "        # Extract feature handling information\n",
    "        feature_handling = design_data['feature_handling']\n",
    "        features = list(feature_handling.keys())\n",
    "        \n",
    "        # Determine feature types and imputation strategies\n",
    "        numerical_features = []\n",
    "        categorical_features = []\n",
    "        imputation = {}\n",
    "        \n",
    "        for feature, details in feature_handling.items():\n",
    "            feature_type = details.get('feature_variable_type', 'numerical')\n",
    "            if feature_type == 'numerical':\n",
    "                numerical_features.append(feature)\n",
    "                feature_details = details.get('feature_details', {})\n",
    "                missing_strategy = feature_details.get('missing_values', 'mean')\n",
    "                imputation_strategy = 'mean' if missing_strategy == 'Impute' else 'median'\n",
    "                imputation[feature] = imputation_strategy\n",
    "            elif feature_type == 'text':\n",
    "                categorical_features.append(feature)\n",
    "            else:\n",
    "                print(f\"Unknown feature type for {feature}: {feature_type}\")\n",
    "        \n",
    "        # Extract feature reduction details\n",
    "        feature_reduction = design_data.get('feature_reduction', {})\n",
    "        \n",
    "        # Extract algorithms\n",
    "        algorithms = design_data.get('algorithms', {})\n",
    "        \n",
    "        return {\n",
    "            'target': target,\n",
    "            'prediction_type': prediction_type,\n",
    "            'numerical_features': numerical_features,\n",
    "            'categorical_features': categorical_features,\n",
    "            'imputation': imputation,\n",
    "            'feature_reduction': feature_reduction,\n",
    "            'algorithms': algorithms\n",
    "        }\n",
    "    except KeyError as e:\n",
    "        print(f\"Missing key in JSON configuration: {e}\")\n",
    "        raise\n",
    "\n",
    "# Function to load data\n",
    "def load_data(csv_file, features, target):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    # Verify that all features and target exist in the DataFrame\n",
    "    missing_features = [f for f in features if f not in df.columns]\n",
    "    if missing_features:\n",
    "        raise KeyError(f\"The following features are missing in the CSV file: {missing_features}\")\n",
    "    \n",
    "    if target not in df.columns:\n",
    "        raise KeyError(f\"The target column '{target}' is missing in the CSV file.\")\n",
    "    \n",
    "    X = df[features]\n",
    "    y = df[target]\n",
    "    return X, y\n",
    "\n",
    "# Function to build preprocessing pipelines\n",
    "def build_preprocessor(numerical_features, categorical_features, imputation):\n",
    "    # Numerical pipeline\n",
    "    numerical_pipeline = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='mean')),  # Default strategy; will be overridden\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "    \n",
    "    # Update imputation strategy for numerical features\n",
    "    # Create a list of tuples for imputation strategies\n",
    "    transformers = []\n",
    "    if numerical_features:\n",
    "        # If different features have different imputation strategies, use a custom imputer\n",
    "        # Otherwise, use a single SimpleImputer\n",
    "        if len(set(imputation.values())) > 1:\n",
    "            # Different imputation strategies; need to handle individually\n",
    "            # This is complex; for simplicity, we'll assume a single strategy\n",
    "            # Alternatively, use separate pipelines for each strategy\n",
    "            numerical_pipeline = Pipeline(steps=[\n",
    "                ('imputer', SimpleImputer(strategy='mean')),\n",
    "                ('scaler', StandardScaler())\n",
    "            ])\n",
    "        else:\n",
    "            # Single imputation strategy\n",
    "            numerical_pipeline = Pipeline(steps=[\n",
    "                ('imputer', SimpleImputer(strategy=list(set(imputation.values())).pop())),\n",
    "                ('scaler', StandardScaler())\n",
    "            ])\n",
    "        \n",
    "        transformers.append(('num', numerical_pipeline, numerical_features))\n",
    "    \n",
    "    # Categorical pipeline\n",
    "    categorical_pipeline = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "    \n",
    "    if categorical_features:\n",
    "        transformers.append(('cat', categorical_pipeline, categorical_features))\n",
    "    \n",
    "    preprocessor = ColumnTransformer(transformers=transformers)\n",
    "    return preprocessor\n",
    "\n",
    "# Function to perform feature reduction\n",
    "def apply_feature_reduction(X, y, feature_reduction_config):\n",
    "    method = feature_reduction_config.get('feature_reduction_method', 'No Reduction')\n",
    "    \n",
    "    if method == 'No Reduction':\n",
    "        return X, None\n",
    "    \n",
    "    elif method == 'Corr with Target':\n",
    "        correlation = X.apply(lambda col: col.corr(y))\n",
    "        selected_features = correlation[correlation.abs() > 0.1].index.tolist()\n",
    "        X_reduced = X[selected_features]\n",
    "        return X_reduced, None\n",
    "    \n",
    "    elif method == 'Tree-based':\n",
    "        model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "        model.fit(X, y)\n",
    "        selector = SelectFromModel(model, threshold=\"mean\", prefit=True)\n",
    "        X_reduced = selector.transform(X)\n",
    "        return X_reduced, selector\n",
    "    \n",
    "    elif method == 'PCA':\n",
    "        pca = PCA(n_components=0.95, random_state=42)\n",
    "        X_reduced = pca.fit_transform(X)\n",
    "        return X_reduced, pca\n",
    "    \n",
    "    else:\n",
    "        print(f\"Unknown feature reduction method: {method}\")\n",
    "        return X, None\n",
    "\n",
    "# Function to get model and its hyperparameter grid based on configuration\n",
    "def get_model_and_params(algorithm_name, algorithm_config, prediction_type):\n",
    "    if not algorithm_config.get('is_selected', False):\n",
    "        return None, None\n",
    "    \n",
    "    if prediction_type == 'regression':\n",
    "        if 'RandomForestRegressor' in algorithm_name:\n",
    "            model = RandomForestRegressor(\n",
    "                n_estimators=algorithm_config.get('min_trees', 100),\n",
    "                max_depth=algorithm_config.get('max_depth', None),\n",
    "                min_samples_leaf=algorithm_config.get('min_samples_per_leaf_min_value', 1),\n",
    "                random_state=42\n",
    "            )\n",
    "            param_grid = {\n",
    "                'n_estimators': [algorithm_config.get('min_trees', 100), algorithm_config.get('max_trees', 200)],\n",
    "                'max_depth': [algorithm_config.get('min_depth', None), algorithm_config.get('max_depth', None)],\n",
    "                'min_samples_leaf': [algorithm_config.get('min_samples_per_leaf_min_value', 1),\n",
    "                                     algorithm_config.get('min_samples_per_leaf_max_value', 4)]\n",
    "            }\n",
    "            return model, param_grid\n",
    "        \n",
    "        elif 'LinearRegression' in algorithm_name:\n",
    "            model = LinearRegression()\n",
    "            param_grid = {\n",
    "                'fit_intercept': [True, False],\n",
    "                'normalize': [True, False]\n",
    "            }\n",
    "            return model, param_grid\n",
    "        \n",
    "        elif 'SVR' in algorithm_name:\n",
    "            model = SVR()\n",
    "            param_grid = {\n",
    "                'kernel': ['linear', 'rbf'],\n",
    "                'C': [1, 10, 100],\n",
    "                'gamma': ['scale', 'auto']\n",
    "            }\n",
    "            return model, param_grid\n",
    "        \n",
    "        elif 'GradientBoostingRegressor' in algorithm_name:\n",
    "            model = GradientBoostingRegressor(random_state=42)\n",
    "            param_grid = {\n",
    "                'n_estimators': [100, 200],\n",
    "                'learning_rate': [0.05, 0.1],\n",
    "                'max_depth': [3, 5],\n",
    "                'subsample': [0.8, 1.0]\n",
    "            }\n",
    "            return model, param_grid\n",
    "        \n",
    "        elif 'DecisionTreeRegressor' in algorithm_name:\n",
    "            model = DecisionTreeRegressor(random_state=42)\n",
    "            param_grid = {\n",
    "                'max_depth': [None, 10, 20],\n",
    "                'min_samples_split': [2, 5, 10],\n",
    "                'min_samples_leaf': [1, 2, 4]\n",
    "            }\n",
    "            return model, param_grid\n",
    "        \n",
    "        else:\n",
    "            print(f\"Unsupported algorithm for regression: {algorithm_name}\")\n",
    "            return None, None\n",
    "    \n",
    "    elif prediction_type == 'classification':\n",
    "        # Implement classification models similarly\n",
    "        pass\n",
    "    \n",
    "    else:\n",
    "        print(f\"Unsupported prediction type: {prediction_type}\")\n",
    "        return None, None\n",
    "\n",
    "# Function to perform hyperparameter tuning\n",
    "def perform_grid_search(pipeline, param_grid, X_train, y_train):\n",
    "    grid_search = GridSearchCV(pipeline, param_grid, cv=5, n_jobs=-1, scoring='neg_mean_squared_error', verbose=1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    return grid_search.best_estimator_, grid_search.best_params_\n",
    "\n",
    "# Function to evaluate the model\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    print(f\"Model Evaluation Metrics:\")\n",
    "    print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "    print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "    print(f\"R-squared (R2): {r2}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "# Main function to run the pipeline\n",
    "def run_pipeline(json_file_path, csv_file):\n",
    "    # Load configuration from the JSON file\n",
    "    config = load_config_from_file(json_file_path)\n",
    "    \n",
    "    # Parse configuration\n",
    "    parsed_config = parse_config(config)\n",
    "    target = parsed_config['target']\n",
    "    prediction_type = parsed_config['prediction_type']\n",
    "    numerical_features = parsed_config['numerical_features']\n",
    "    categorical_features = parsed_config['categorical_features']\n",
    "    imputation = parsed_config['imputation']\n",
    "    feature_reduction = parsed_config['feature_reduction']\n",
    "    algorithms = parsed_config['algorithms']\n",
    "    \n",
    "    print(\"Parsed Configuration:\")\n",
    "    print(f\"Target: {target}\")\n",
    "    print(f\"Prediction Type: {prediction_type}\")\n",
    "    print(f\"Numerical Features: {numerical_features}\")\n",
    "    print(f\"Categorical Features: {categorical_features}\")\n",
    "    print(f\"Imputation Strategies: {imputation}\")\n",
    "    print(f\"Feature Reduction: {feature_reduction}\")\n",
    "    print(f\"Algorithms: {algorithms}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Load Data\n",
    "    X, y = load_data(csv_file, numerical_features + categorical_features, target)\n",
    "    \n",
    "    # Build Preprocessor\n",
    "    preprocessor = build_preprocessor(numerical_features, categorical_features, imputation)\n",
    "    \n",
    "    # Create a list to hold all trained models\n",
    "    trained_models = []\n",
    "    \n",
    "    # Iterate over each selected algorithm\n",
    "    for algo_name, algo_config in algorithms.items():\n",
    "        if not algo_config.get('is_selected', False):\n",
    "            continue  # Skip if not selected\n",
    "        \n",
    "        print(f\"Processing Algorithm: {algo_name}\")\n",
    "        \n",
    "        # Get model and hyperparameter grid\n",
    "        model, param_grid = get_model_and_params(algo_name, algo_config, prediction_type)\n",
    "        if model is None:\n",
    "            continue  # Unsupported or not selected\n",
    "        \n",
    "        # Create a pipeline with preprocessor and model\n",
    "        pipeline = Pipeline(steps=[\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('feature_generator', FeatureGenerator(config)),  # Placeholder for feature generation\n",
    "            ('model', model)\n",
    "        ])\n",
    "        \n",
    "        # Define full parameter grid with pipeline steps\n",
    "        full_param_grid = {}\n",
    "        for param, values in param_grid.items():\n",
    "            full_param_grid[f'model__{param}'] = values\n",
    "        \n",
    "        # Perform Grid Search\n",
    "        print(f\"Starting Grid Search for {algo_name}...\")\n",
    "        best_pipeline, best_params = perform_grid_search(pipeline, full_param_grid, X, y)\n",
    "        print(f\"Best Parameters for {algo_name}: {best_params}\")\n",
    "        \n",
    "        # Train-Test Split for Evaluation\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        \n",
    "        # Fit the best pipeline on training data\n",
    "        best_pipeline.fit(X_train, y_train)\n",
    "        \n",
    "        # Evaluate the model\n",
    "        evaluate_model(best_pipeline, X_test, y_test)\n",
    "        \n",
    "        # Save the trained model\n",
    "        model_filename = f\"{algo_name}_model.pkl\"\n",
    "        joblib.dump(best_pipeline, model_filename)\n",
    "        print(f\"Saved best model to {model_filename}\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Append to trained models list\n",
    "        trained_models.append((algo_name, best_pipeline))\n",
    "    \n",
    "    print(\"All selected models have been processed and evaluated.\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    json_file_path = r'C:\\Users\\ASUS\\Desktop\\algoparams_from_ui.json'  # Replace with your JSON file path\n",
    "    csv_file = 'iris.csv'  # Replace with your CSV file path\n",
    "    run_pipeline(json_file_path, csv_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748e8947-0104-4cdd-b631-544b27dbae72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948a9a8f-c0ff-4025-9713-8880134cb362",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
